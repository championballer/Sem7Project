{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import mgym \n",
    "import random\n",
    "from mgym.envs.snake_env import SnakeEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(30, 30)\n",
      "Tuple(Discrete(4))\n"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(30,30)\n",
    "env.reset(1)\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class QNetwork(nn.Module):\n",
    "#     \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "#     def __init__(self, state, action_size,hidden_layers,seed,drop_p=0.5):\n",
    "#         \"\"\"Initialize parameters and build model.\n",
    "#         Params\n",
    "#         ======\n",
    "#             state_size (int): Dimension of each state\n",
    "#             hidden layers : takes in a list of hidden layer dimensions, to be able to initialise a model with multiple layers\n",
    "#             action_size (int): Dimension of each action\n",
    "#             seed (int): Random seed\n",
    "#         \"\"\"\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "#         self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "#         self.hidden_layers = nn.ModuleList([nn.Linear(state_size,hidden_layers[0])])\n",
    "#         self.hidden_layers.extend([nn.Linear(h1, h2) for h1,h2 in zip(hidden_layers[:-1], hidden_layers[1:])]) \n",
    "        \n",
    "#         self.output = nn.Linear(hidden_layers[-1],action_size)\n",
    "#         #self.dropout = nn.Dropout(p=drop_p)\n",
    "                                             \n",
    "#     def forward(self, state):\n",
    "#         \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "#         x = state\n",
    "#         for linear in self.hidden_layers:\n",
    "#             x = F.relu(linear(x))\n",
    "#             #x = self.dropout(x)\n",
    "        \n",
    "#         x = self.output(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# from collections import namedtuple, deque\n",
    "\n",
    "# from model2 import QNetwork\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "# BATCH_SIZE = 64         # minibatch size\n",
    "# GAMMA = 0.99            # discount factor\n",
    "# TAU = 1e-3              # for soft update of target parameters\n",
    "# LR = 5e-4               # learning rate \n",
    "# UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Agent():\n",
    "#     \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "#     def __init__(self, state_size, action_size, seed):\n",
    "#         \"\"\"Initialize an Agent object.\n",
    "        \n",
    "#         Params\n",
    "#         ======\n",
    "#             state_size (int): dimension of each state\n",
    "#             action_size (int): dimension of each action\n",
    "#             seed (int): random seed\n",
    "#         \"\"\"\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.seed = random.seed(seed)\n",
    "\n",
    "#         # Q-Network\n",
    "#         hidden_layers = [128,64]\n",
    "#         self.qnetwork_local = QNetwork(state_size, action_size, hidden_layers, seed).to(device)\n",
    "#         self.qnetwork_target = QNetwork(state_size, action_size, hidden_layers, seed).to(device)\n",
    "#         self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "#         # Replay memory\n",
    "#         self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "#         # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "#         self.t_step = 0\n",
    "    \n",
    "#     def step(self, state, action, reward, next_state, done):\n",
    "#         # Save experience in replay memory\n",
    "#         self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "#         # Learn every UPDATE_EVERY time steps.\n",
    "#         self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "#         if self.t_step == 0:\n",
    "#             # If enough samples are available in memory, get random subset and learn\n",
    "#             if len(self.memory) > BATCH_SIZE:\n",
    "#                 experiences = self.memory.sample()\n",
    "#                 self.learn(experiences, GAMMA)\n",
    "\n",
    "#     def act(self, state, eps=0.):\n",
    "#         \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "#         Params\n",
    "#         ======\n",
    "#             state (array_like): current state\n",
    "#             eps (float): epsilon, for epsilon-greedy action selection\n",
    "#         \"\"\"\n",
    "#         state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "#         self.qnetwork_local.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action_values = self.qnetwork_local(state)\n",
    "#         self.qnetwork_local.train()\n",
    "\n",
    "#         # Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "\n",
    "#     def learn(self, experiences, gamma):\n",
    "#         \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "#             gamma (float): discount factor\n",
    "#         \"\"\"\n",
    "#         states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \n",
    "#         max_actions = self.qnetwork_local.forward(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "#         output_target = self.qnetwork_target.forward(next_states).gather(1,max_actions)\n",
    "#         td_target = rewards + gamma*(output_target*(1-dones))\n",
    "#         output_local= self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "#         loss = F.mse_loss(output_local,td_target)\n",
    "        \n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         # ------------------- update target network ------------------- #\n",
    "#         self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "#     def soft_update(self, local_model, target_model, tau):\n",
    "#         \"\"\"Soft update model parameters.\n",
    "#         θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             local_model (PyTorch model): weights will be copied from\n",
    "#             target_model (PyTorch model): weights will be copied to\n",
    "#             tau (float): interpolation parameter \n",
    "#         \"\"\"\n",
    "#         for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "#             target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "# class ReplayBuffer:\n",
    "#     \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "#     def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "#         \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             action_size (int): dimension of each action\n",
    "#             buffer_size (int): maximum size of buffer\n",
    "#             batch_size (int): size of each training batch\n",
    "#             seed (int): random seed\n",
    "#         \"\"\"\n",
    "#         self.action_size = action_size\n",
    "#         self.memory = deque(maxlen=buffer_size)  \n",
    "#         self.batch_size = batch_size\n",
    "#         self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "#         self.seed = random.seed(seed)\n",
    "    \n",
    "#     def add(self, state, action, reward, next_state, done):\n",
    "#         \"\"\"Add a new experience to memory.\"\"\"\n",
    "#         e = self.experience(state, action, reward, next_state, done)\n",
    "#         self.memory.append(e)\n",
    "    \n",
    "#     def sample(self):\n",
    "#         \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "#         states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "#         actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "#         rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "#         next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "#         dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "#         return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Return the current size of internal memory.\"\"\"\n",
    "#         return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=512, fc2_units=256,fc3_units=128,fc4_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor,self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=0)\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(5184, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units,fc4_units)\n",
    "        self.output = nn.Linear(fc4_units, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.output.weight.data.uniform_(*hidden_init(self.output))\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        \n",
    "        \n",
    "        x = self.convlayer1(state.reshape(-1,1,state.shape[0],state.shape[1]))\n",
    "        x = self.convlayer2(x)\n",
    "        #print(x.shape)\n",
    "        #print(x.reshape(x.shape[2]*x.shape[3]).shape)\n",
    "        x = F.relu(self.fc1(x.reshape(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        output = self.output(x)\n",
    "        return self.softmax(output)\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=512, fc2_units=256,fc3_units=128,fc4_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "        self.convlayer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.convlayer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc1 = nn.Linear(5184, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units,fc3_units)\n",
    "        self.fc4 = nn.Linear(fc3_units,fc4_units)\n",
    "        self.output = nn.Linear(fc4_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(*hidden_init(self.fc3))\n",
    "        self.fc4.weight.data.uniform_(*hidden_init(self.fc4))\n",
    "        self.output.weight.data.uniform_(*hidden_init(self.output))\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state) pairs -> Q-values.\"\"\"\n",
    "        x = self.convlayer1(state.reshape(-1,1,state.shape[0],state.shape[1]))\n",
    "        x = self.convlayer2(x)\n",
    "        x = F.relu(self.fc1(x.reshape(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])))\n",
    "        xs = F.relu(self.fc2(x))\n",
    "        xs = F.relu(self.fc3(xs))\n",
    "        xs = F.relu(self.fc4(xs))\n",
    "        output = F.relu(self.output(xs))\n",
    "        return torch.max(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_actor = Actor(100,4,1)\n",
    "test_critic = Critic(100,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (convlayer1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convlayer2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc1): Linear(in_features=5184, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (fc4): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_actor.double()\n",
    "test_critic.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.5775, dtype=torch.float64, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_actor.forward(torch.from_numpy(env.grid))\n",
    "test_critic.forward(torch.from_numpy(env.grid),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4         # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed,num_agents=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "            num_agents (int) : number of agents in the environment \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Base Working for multiple agents\n",
    "        ======\n",
    "        \n",
    "        Many different agents will sample the environment at the same time to get different states, \n",
    "        for which based on the current policy actions will be decided, rewards will be received along with\n",
    "        the next states. All the agents update the same experience replay buffer and utilise the same neural \n",
    "        net to decide on the optimal set of actions. This should theoretically increase training efficiency \n",
    "        since so many different states are being experienced at the same time.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents,action_size),random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        #Initial target and local networks with same weights (Student Hub Discussion)\n",
    "        self.hard_update(self.actor_local,self.actor_target)\n",
    "        self.hard_update(self.critic_local,self.critic_target)\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for state, action, reward, next_state, done in zip(states, actions,rewards,next_states,dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    \"\"\"To decouple learning from experience collection and use random sample from buffer to learn.\"\"\"\n",
    "    def update(self):\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        return np.random.choice(4,p=action)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(),1)\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def hard_update(self,local_model,target_model):\n",
    "        \n",
    "        for target_param,local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size,seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128.\n",
      "  128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128.\n",
      "  128. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.  64.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0. 252.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.  64.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "    0. 128.]\n",
      " [128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128.\n",
      "  128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128. 128.\n",
      "  128. 128.]]\n"
     ]
    }
   ],
   "source": [
    "print(env.grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake1 = Agent(900,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action = snake1.act(env.grid,0.5)\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAKE 1 DIED!\n",
      "\n",
      "\n",
      "*******************\n",
      "**** GAME OVER ****\n",
      "******************* \n",
      "\n",
      "episode: 999 max reward: 3\n",
      "{1: 964, 2: 34, 3: 1}\n"
     ]
    }
   ],
   "source": [
    "timesteps = 400\n",
    "epochs = 1000\n",
    "max_reward = 0\n",
    "rewards = {}\n",
    "game_images = {}\n",
    "epsilon = 1.\n",
    "for i in range(epochs):\n",
    "    \n",
    "    if i%100==0:\n",
    "        epsilon=max(0.01,epsilon-0.00001)\n",
    "    iterations = 0\n",
    "    env.reset(1)\n",
    "    images = []\n",
    "    while True:\n",
    "        a = tuple([snake1.act(env.grid,epsilon)])\n",
    "        #print(a)\n",
    "        #print(type(a))\n",
    "        state,reward,done,_ = env.step(a)\n",
    "        iterations+=1\n",
    "        images.append(env.grid)\n",
    "#         if iterations%10==0:\n",
    "#             print(\"epoch:\",i)\n",
    "#             print(\"timestep:\",iterations)\n",
    "#             plt.imshow(state)\n",
    "#             plt.show()\n",
    "\n",
    "        if done or iterations>timesteps:\n",
    "            if reward[0]>max_reward:\n",
    "                max_reward = reward[0]\n",
    "                \n",
    "            print(\"episode:\",i,\"max reward:\",max_reward)\n",
    "            print(rewards)\n",
    "            if reward[0] not in rewards.keys():\n",
    "                rewards[reward[0]] = 1\n",
    "                game_images[reward[0]] = images\n",
    "            else:\n",
    "                rewards[reward[0]] += 1\n",
    "                game_images[reward[0]] = images\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "* It is possible that after some time, due to the lack of difference between the values of the apple and the snake element, that the snake thinks that the closest fruit is in its body itself. A possible solution to that could be that we make the snake element and fruit element values to be drastically different. To avoid this confusion.\n",
    "\n",
    "* The reward structure can also be changed. Right now we start with 1 reward, don't penalise for dying and don't reward for staying alive, plus give reward of 1 for finding the fruit. The reward structure can be changed to heavily penalising death, giving more rewards for eating an apple, and also incentivise staying alive. \n",
    "\n",
    "* Plus we started with only MLP, which in my personal opinion is not scalable, as the size of the grid increases, it will be harder for the agent to process the complete space, and hence it is important for it to be able to derive features from the picture, and that can be done through a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAACw1JREFUeJzt3V+IpYV5x/HvL2pu1IW1DstipJuKFEKha5iaQiSkpAbjjXoT4kXYgrC5UFDIRSWlVOiNlGjoRRHWKtkWayiouBfSxooggSKOstXVpdXKhuyy7o54sZurdPXpxbwLk+3uzNk5f96xz/cDhznnPe+Z9+HFr+e873vYSVUhqZ8vjD2ApHEYv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNXTnNi5PcAfwtcAXw91X16EbrX3/9tbVnz9I0m5S0gWPHVvn447OZZN0tx5/kCuDvgNuB48AbSQ5V1XuXes2ePUu8/sZfb3WTkjbxtT/6y4nXneZj/63AB1X1YVX9BvgZcNcUv0/SAk0T/w3Ar9Y9Pj4sk/Q5MPcTfkn2J1lJsrK6embem5M0oWniPwHcuO7xl4Zlv6WqDlTVclUtLy3tmGJzkmZpmvjfAG5O8uUkXwS+BxyazViS5m3LZ/ur6lySB4B/Ze1S39NV9e6WB/nCvq2+VGrn3GcHp/4dU13nr6qXgJemnkLSwvkNP6kp45eaMn6pKeOXmjJ+qSnjl5qa6lKf/n94+Wv3XPK5219/YYGTaJF855eaMn6pKeOXmjJ+qSnjl5oyfqkpL/XJy3nb0EaXX2fFd36pKeOXmjJ+qSnjl5oyfqkp45ea8lKftA1tdvn1HHdPvQ3f+aWmjF9qyvilpoxfasr4paaMX2rK+KWmprrOn+QYcBb4FDhXVcuzGErS/M3iSz5/UlUfz+D3SFogP/ZLTU0bfwE/T/Jmkv0XWyHJ/iQrSVZWV89MuTlJszJt/LdV1VeB7wD3J/nGhStU1YGqWq6q5aWlHVNuTtKsTBV/VZ0Yfp4GXgBuncVQkuZvy/EnuTrJtefvA98GjsxqMEnzNc3Z/l3AC0nO/55/qqp/mclUkuZuy/FX1YfAH85wFkkL5KU+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqmpTeNP8nSS00mOrFt2XZKXk7w//Nw53zElzdok7/w/Be64YNnDwCtVdTPwyvBY0ufIpvFX1WvAJxcsvgs4ONw/CNw947kkzdlWj/l3VdXJ4f5HwK5LrZhkf5KVJCurq2e2uDlJszb1Cb+qKqA2eP5AVS1X1fLS0o5pNydpRrYa/6kkuwGGn6dnN5KkRdhq/IeAfcP9fcCLsxlH0qJMcqnvWeDfgd9PcjzJfcCjwO1J3gf+dHgs6XPkys1WqKp7L/HUt2Y8i6QF8ht+UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTxi81ZfxSU8YvNWX8UlPGLzVl/FJTk/yhzqeTnE5yZN2yR5KcSHJ4uN053zElzdok7/w/Be64yPKfVNXe4fbSbMeSNG+bxl9VrwGfLGAWSQs0zTH/A0neHg4Lds5sIkkLsdX4nwBuAvYCJ4HHLrVikv1JVpKsrK6e2eLmJM3aluKvqlNV9WlVfQY8Cdy6wboHqmq5qpaXlnZsdU5JM7al+JPsXvfwHuDIpdaVtD1dudkKSZ4Fvglcn+Q48FfAN5PsBQo4BvxgjjNKmoNN46+qey+y+Kk5zCJpgfyGn9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/U1KbxJ7kxyatJ3kvybpIHh+XXJXk5yfvDz53zH1fSrEzyzn8O+GFVfQX4Y+D+JF8BHgZeqaqbgVeGx5I+JzaNv6pOVtVbw/2zwFHgBuAu4OCw2kHg7nkNKWn2LuuYP8ke4BbgdWBXVZ0cnvoI2DXTySTN1cTxJ7kGeA54qKrOrH+uqgqoS7xuf5KVJCurq2cutoqkEUwUf5KrWAv/map6flh8Ksnu4fndwOmLvbaqDlTVclUtLy3tmMXMkmZgkrP9AZ4CjlbV4+ueOgTsG+7vA16c/XiS5uXKCdb5OvB94J0kh4dlPwIeBf45yX3AL4HvzmdESfOwafxV9Qsgl3j6W7MdR9Ki+A0/qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eaMn6pKeOXmjJ+qSnjl5oyfqkp45eamuRPdN+Y5NUk7yV5N8mDw/JHkpxIcni43Tn/cSXNyiR/ovsc8MOqeivJtcCbSV4envtJVf14fuNJmpdJ/kT3SeDkcP9skqPADfMeTNJ8XdYxf5I9wC3A68OiB5K8neTpJDsv8Zr9SVaSrKyunplqWEmzM3H8Sa4BngMeqqozwBPATcBe1j4ZPHax11XVgaparqrlpaUdMxhZ0ixMFH+Sq1gL/5mqeh6gqk5V1adV9RnwJHDr/MaUNGuTnO0P8BRwtKoeX7d897rV7gGOzH48SfMyydn+rwPfB95JcnhY9iPg3iR7gQKOAT+Yy4SS5mKSs/2/AHKRp16a/TiSFsVv+ElNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/UlPFLTRm/1JTxS00Zv9SU8UtNGb/U1CT/eu9CnPvs4NgjSK34zi81ZfxSU8YvNWX8UlPGLzVl/FJTqarFbSxZBX65btH1wMcLG2BzzrOx7TYPbL+Zxp7nd6tqaZIVFxr//9l4slJVy6MNcAHn2dh2mwe230zbbZ6N+LFfasr4pabGjv/AyNu/kPNsbLvNA9tvpu02zyWNeswvaTxjv/NLGsko8Se5I8l/JvkgycNjzHDBPMeSvJPkcJKVkWZ4OsnpJEfWLbsuyctJ3h9+7hx5nkeSnBj20+Ekdy5wnhuTvJrkvSTvJnlwWD7KPtpgntH20eVa+Mf+JFcA/wXcDhwH3gDurar3FjrIb890DFiuqtGuzyb5BvBr4B+q6g+GZX8DfFJVjw7/k9xZVX8+4jyPAL+uqh8vYoYL5tkN7K6qt5JcC7wJ3A38GSPsow3m+S4j7aPLNcY7/63AB1X1YVX9BvgZcNcIc2wrVfUa8MkFi+8Czv9DBwdZ+49rzHlGU1Unq+qt4f5Z4ChwAyPtow3m+dwYI/4bgF+te3yc8XdaAT9P8maS/SPPst6uqjo53P8I2DXmMIMHkrw9HBYs7DBkvSR7gFuA19kG++iCeWAb7KNJeMJvzW1V9VXgO8D9w0febaXWjs/GvjTzBHATsBc4CTy26AGSXAM8BzxUVWfWPzfGPrrIPKPvo0mNEf8J4MZ1j780LBtNVZ0Yfp4GXmDt0GQ7ODUcW54/xjw95jBVdaqqPq2qz4AnWfB+SnIVa6E9U1XPD4tH20cXm2fsfXQ5xoj/DeDmJF9O8kXge8ChEeYAIMnVwwkbklwNfBs4svGrFuYQsG+4vw94ccRZzsd13j0scD8lCfAUcLSqHl/31Cj76FLzjLmPLltVLfwG3MnaGf//Bv5ijBnWzfJ7wH8Mt3fHmgd4lrWPif/D2nmQ+4DfAV4B3gf+Dbhu5Hn+EXgHeJu16HYvcJ7bWPtI/zZweLjdOdY+2mCe0fbR5d78hp/UlCf8pKaMX2rK+KWmjF9qyvilpoxfasr4paaMX2rqfwHVmmlQXX2FlQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(game_images[3])):\n",
    "    \n",
    "    img = game_images[3][i]\n",
    "    plt.imshow(img,cmap='inferno')\n",
    "    #plt.savefig('./Output/output'+str(i)+'.png')\n",
    "    plt.show()\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0877,  0.0418,  0.0182, -0.0191], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action = Agent.forward(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(4, p=[0.3, 0.2, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
