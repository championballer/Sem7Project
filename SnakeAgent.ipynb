{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import mgym \n",
    "import random\n",
    "from mgym.envs.snake_env import SnakeEnv\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(17, 17)\n",
      "Tuple(Discrete(4))\n"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv(17,17)\n",
    "env.reset(1)\n",
    "\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.grid)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# class QNetwork(nn.Module):\n",
    "#     \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "#     def __init__(self, state, action_size,hidden_layers,seed,drop_p=0.5):\n",
    "#         \"\"\"Initialize parameters and build model.\n",
    "#         Params\n",
    "#         ======\n",
    "#             state_size (int): Dimension of each state\n",
    "#             hidden layers : takes in a list of hidden layer dimensions, to be able to initialise a model with multiple layers\n",
    "#             action_size (int): Dimension of each action\n",
    "#             seed (int): Random seed\n",
    "#         \"\"\"\n",
    "#         super(QNetwork, self).__init__()\n",
    "#         self.seed = torch.manual_seed(seed)\n",
    "#         self.conv1 = torch.nn.Conv2d(3, 18, kernel_size=3, stride=1, padding=1)\n",
    "#         self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "#         self.hidden_layers = nn.ModuleList([nn.Linear(state_size,hidden_layers[0])])\n",
    "#         self.hidden_layers.extend([nn.Linear(h1, h2) for h1,h2 in zip(hidden_layers[:-1], hidden_layers[1:])]) \n",
    "        \n",
    "#         self.output = nn.Linear(hidden_layers[-1],action_size)\n",
    "#         #self.dropout = nn.Dropout(p=drop_p)\n",
    "                                             \n",
    "#     def forward(self, state):\n",
    "#         \"\"\"Build a network that maps state -> action values.\"\"\"\n",
    "#         x = state\n",
    "#         for linear in self.hidden_layers:\n",
    "#             x = F.relu(linear(x))\n",
    "#             #x = self.dropout(x)\n",
    "        \n",
    "#         x = self.output(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import random\n",
    "# from collections import namedtuple, deque\n",
    "\n",
    "# from model2 import QNetwork\n",
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "# BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "# BATCH_SIZE = 64         # minibatch size\n",
    "# GAMMA = 0.99            # discount factor\n",
    "# TAU = 1e-3              # for soft update of target parameters\n",
    "# LR = 5e-4               # learning rate \n",
    "# UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# class Agent():\n",
    "#     \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "#     def __init__(self, state_size, action_size, seed):\n",
    "#         \"\"\"Initialize an Agent object.\n",
    "        \n",
    "#         Params\n",
    "#         ======\n",
    "#             state_size (int): dimension of each state\n",
    "#             action_size (int): dimension of each action\n",
    "#             seed (int): random seed\n",
    "#         \"\"\"\n",
    "#         self.state_size = state_size\n",
    "#         self.action_size = action_size\n",
    "#         self.seed = random.seed(seed)\n",
    "\n",
    "#         # Q-Network\n",
    "#         hidden_layers = [128,64]\n",
    "#         self.qnetwork_local = QNetwork(state_size, action_size, hidden_layers, seed).to(device)\n",
    "#         self.qnetwork_target = QNetwork(state_size, action_size, hidden_layers, seed).to(device)\n",
    "#         self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "#         # Replay memory\n",
    "#         self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "#         # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "#         self.t_step = 0\n",
    "    \n",
    "#     def step(self, state, action, reward, next_state, done):\n",
    "#         # Save experience in replay memory\n",
    "#         self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "#         # Learn every UPDATE_EVERY time steps.\n",
    "#         self.t_step = (self.t_step + 1) % UPDATE_EVERY\n",
    "#         if self.t_step == 0:\n",
    "#             # If enough samples are available in memory, get random subset and learn\n",
    "#             if len(self.memory) > BATCH_SIZE:\n",
    "#                 experiences = self.memory.sample()\n",
    "#                 self.learn(experiences, GAMMA)\n",
    "\n",
    "#     def act(self, state, eps=0.):\n",
    "#         \"\"\"Returns actions for given state as per current policy.\n",
    "        \n",
    "#         Params\n",
    "#         ======\n",
    "#             state (array_like): current state\n",
    "#             eps (float): epsilon, for epsilon-greedy action selection\n",
    "#         \"\"\"\n",
    "#         state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "#         self.qnetwork_local.eval()\n",
    "#         with torch.no_grad():\n",
    "#             action_values = self.qnetwork_local(state)\n",
    "#         self.qnetwork_local.train()\n",
    "\n",
    "#         # Epsilon-greedy action selection\n",
    "#         if random.random() > eps:\n",
    "#             return np.argmax(action_values.cpu().data.numpy())\n",
    "#         else:\n",
    "#             return random.choice(np.arange(self.action_size))\n",
    "\n",
    "#     def learn(self, experiences, gamma):\n",
    "#         \"\"\"Update value parameters using given batch of experience tuples.\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             experiences (Tuple[torch.Variable]): tuple of (s, a, r, s', done) tuples \n",
    "#             gamma (float): discount factor\n",
    "#         \"\"\"\n",
    "#         states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        \n",
    "#         max_actions = self.qnetwork_local.forward(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "#         output_target = self.qnetwork_target.forward(next_states).gather(1,max_actions)\n",
    "#         td_target = rewards + gamma*(output_target*(1-dones))\n",
    "#         output_local= self.qnetwork_local(states).gather(1, actions)\n",
    "        \n",
    "#         loss = F.mse_loss(output_local,td_target)\n",
    "        \n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "\n",
    "#         # ------------------- update target network ------------------- #\n",
    "#         self.soft_update(self.qnetwork_local, self.qnetwork_target, TAU)                     \n",
    "\n",
    "#     def soft_update(self, local_model, target_model, tau):\n",
    "#         \"\"\"Soft update model parameters.\n",
    "#         θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             local_model (PyTorch model): weights will be copied from\n",
    "#             target_model (PyTorch model): weights will be copied to\n",
    "#             tau (float): interpolation parameter \n",
    "#         \"\"\"\n",
    "#         for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "#             target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "\n",
    "\n",
    "# class ReplayBuffer:\n",
    "#     \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "#     def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "#         \"\"\"Initialize a ReplayBuffer object.\n",
    "\n",
    "#         Params\n",
    "#         ======\n",
    "#             action_size (int): dimension of each action\n",
    "#             buffer_size (int): maximum size of buffer\n",
    "#             batch_size (int): size of each training batch\n",
    "#             seed (int): random seed\n",
    "#         \"\"\"\n",
    "#         self.action_size = action_size\n",
    "#         self.memory = deque(maxlen=buffer_size)  \n",
    "#         self.batch_size = batch_size\n",
    "#         self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "#         self.seed = random.seed(seed)\n",
    "    \n",
    "#     def add(self, state, action, reward, next_state, done):\n",
    "#         \"\"\"Add a new experience to memory.\"\"\"\n",
    "#         e = self.experience(state, action, reward, next_state, done)\n",
    "#         self.memory.append(e)\n",
    "    \n",
    "#     def sample(self):\n",
    "#         \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "#         experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "#         states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "#         actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "#         rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "#         next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "#         dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "  \n",
    "#         return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         \"\"\"Return the current size of internal memory.\"\"\"\n",
    "#         return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import sqrt\n",
    "\n",
    "#Actor and Critic Networks\n",
    "def hidden_init(layer):\n",
    "    fan_in = layer.weight.data.size()[0]\n",
    "    lim = 1. / np.sqrt(fan_in)\n",
    "    return (-lim, lim)\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed,fc1_units=128,fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "#         self.convlayer1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.convlayer2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2)\n",
    "        self.fc1 = nn.LeakyReLU(state_size, fc1_units)\n",
    "        self.fc2 = nn.LeakyReLU(fc1_units, fc2_units)\n",
    "        self.output = nn.LeakyReLU(fc2_units, action_size)\n",
    "        \n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \n",
    "#         self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "#         self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "#         self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.parameter)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.parameter)\n",
    "        torch.nn.init.kaiming_uniform_(self.output.parameter)\n",
    "#         torch.nn.init.kaiming_uniform_(self.convlayer1.weight)\n",
    "#         torch.nn.init.kaiming_uniform_(self.convlayer2.weight)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> actions.\"\"\"\n",
    "        \n",
    "        \n",
    "#         x = F.relu(self.convlayer1(state.reshape(-1,1,state.shape[0],state.shape[1])))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = F.relu(self.convlayer2(x))\n",
    "#         x = self.maxpool(x)\n",
    "#         #print(x.shape)\n",
    "        #print(x.reshape(x.shape[2]*x.shape[3]).shape)\n",
    "        x = (self.fc1(state.reshape(-1,state.shape[0]*state.shape[1])))\n",
    "        x = (self.fc2(x))\n",
    "        output = (self.output(x))\n",
    "        return output[0]\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=128,fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fcs1_units (int): Number of nodes in the first hidden layer\n",
    "            fc2_units (int): Number of nodes in the second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic,self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        \n",
    "#         self.convlayer1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=2)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#         self.convlayer2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=2)\n",
    "        self.fc1 = nn.LeakyReLU(state_size, fc1_units)\n",
    "        self.fc2 = nn.LeakyReLU(fc1_units, fc2_units)\n",
    "        self.output = nn.Linear(fc2_units, action_size)\n",
    "        self.reset_parameters()\n",
    "\n",
    "        \n",
    "    def kaiming(self,units):\n",
    "        return torch.nn.Parameter(torch.randn(units[0],units[1])*sqrt(2./units[0]))\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "#         self.fc1.weight.data.uniform_(*hidden_init(self.fc1))\n",
    "#         self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\n",
    "#         self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc1.parameter)\n",
    "        torch.nn.init.kaiming_uniform_(self.fc2.parameter)\n",
    "        torch.nn.init.kaiming_uniform_(self.output.parameter)\n",
    "#         torch.nn.init.kaiming_uniform_(self.convlayer1.weight)\n",
    "#         torch.nn.init.kaiming_uniform_(self.convlayer2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a critic (value) network that maps (state) pairs -> Q-values.\"\"\"\n",
    "#         x = F.relu(self.convlayer1(state.reshape(-1,1,state.shape[0],state.shape[1])))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = F.relu(self.convlayer2(x))\n",
    "#         x = self.maxpool(x)\n",
    "#         x = F.relu(self.fc1(x.reshape(x.shape[0]*x.shape[1]*x.shape[2]*x.shape[3])))\n",
    "        x = (self.fc1(state.reshape(-1,state.shape[0]*state.shape[1])))\n",
    "        xs = (self.fc2(x))\n",
    "        output = (self.output(xs))\n",
    "#        print(output)\n",
    "        return output[0][action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LeakyReLU' object has no attribute 'parameter'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-7fa85ad5f596>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_actor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m289\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_critic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m289\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-26c9ad78827f>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, state_size, action_size, seed, fc1_units, fc2_units)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfc2_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-113-26c9ad78827f>\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m#         self.fc2.weight.data.uniform_(*hidden_init(self.fc2))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m#         self.output.weight.data.uniform_(-3e-3, 3e-3)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LeakyReLU' object has no attribute 'parameter'"
     ]
    }
   ],
   "source": [
    "test_actor = Actor(289,4,1)\n",
    "test_critic = Critic(289,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Critic(\n",
       "  (fc1): Linear(in_features=289, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_actor.double()\n",
    "test_critic.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17, 17)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LeakyReLU' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-a4915b6910a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_actor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-34f727a215cd>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m#print(x.reshape(x.shape[2]*x.shape[3]).shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \"\"\"\n\u001b[0;32m-> 1368\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 585\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LeakyReLU' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "print(test_actor.forward(torch.from_numpy(env.grid)))\n",
    "print(test_critic.forward(torch.from_numpy(env.grid),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128    # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-4        # learning rate of the actor \n",
    "LR_CRITIC = 1e-4        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, random_seed,num_agents=1):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        \n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): dimension of each state\n",
    "            action_size (int): dimension of each action\n",
    "            random_seed (int): random seed\n",
    "            num_agents (int) : number of agents in the environment \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        Base Working for multiple agents\n",
    "        ======\n",
    "        \n",
    "        Many different agents will sample the environment at the same time to get different states, \n",
    "        for which based on the current policy actions will be decided, rewards will be received along with\n",
    "        the next states. All the agents update the same experience replay buffer and utilise the same neural \n",
    "        net to decide on the optimal set of actions. This should theoretically increase training efficiency \n",
    "        since so many different states are being experienced at the same time.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(random_seed)\n",
    "\n",
    "        # Actor Network (w/ Target Network)\n",
    "        self.actor_local = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size, random_seed).to(device)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), lr=LR_ACTOR)\n",
    "\n",
    "        # Critic Network (w/ Target Network)\n",
    "        self.critic_local = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_target = Critic(state_size, action_size, random_seed).to(device)\n",
    "        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "        # Noise process\n",
    "        self.noise = OUNoise((num_agents,action_size),random_seed)\n",
    "\n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, random_seed)\n",
    "        \n",
    "        #Initial target and local networks with same weights (Student Hub Discussion)\n",
    "        self.hard_update(self.actor_local,self.actor_target)\n",
    "        self.hard_update(self.critic_local,self.critic_target)\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Save experience in replay memory.\"\"\"\n",
    "        # Save experience / reward\n",
    "        for state, action, reward, next_state, done in zip(states, actions,rewards,next_states,dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "    \n",
    "    \"\"\"To decouple learning from experience collection and use random sample from buffer to learn.\"\"\"\n",
    "    def update(self):\n",
    "        # Learn, if enough samples are available in memory\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def act(self, state, eps, add_noise=True):\n",
    "        \"\"\"Returns actions for given state as per current policy.\"\"\"\n",
    "        state = torch.from_numpy(state).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            action = self.actor_local(state).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        probs = []\n",
    "        for i in range(4):\n",
    "            probs.append(0)\n",
    "        action_max = np.argmax(action)\n",
    "#         print(action_max)\n",
    "#         print(type(epsilon))\n",
    "        probs[action_max] = 1 - epsilon + epsilon/4\n",
    "        for i in range(4):\n",
    "            if i == action_max:\n",
    "                continue\n",
    "            else:\n",
    "                probs[i] = epsilon/4\n",
    "        return np.random.choice(4,p=probs)\n",
    "\n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update policy and value parameters using given batch of experience tuples.\n",
    "        Q_targets = r + γ * critic_target(next_state, actor_target(next_state))\n",
    "        where:\n",
    "            actor_target(state) -> action\n",
    "            critic_target(state, action) -> Q-value\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # ---------------------------- update critic ---------------------------- #\n",
    "        # Get predicted next-state actions and Q values from target models\n",
    "        actions_next = self.actor_target(next_states)\n",
    "        Q_targets_next = self.critic_target(next_states, actions_next)\n",
    "        # Compute Q targets for current states (y_i)\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        # Compute critic loss\n",
    "        Q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        # Minimize the loss\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        torch.nn.utils.clip_grad_norm(self.critic_local.parameters(),1)\n",
    "        # ---------------------------- update actor ---------------------------- #\n",
    "        # Compute actor loss\n",
    "        actions_pred = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_pred).mean()\n",
    "        # Minimize the loss\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ----------------------- update target networks ----------------------- #\n",
    "        self.soft_update(self.critic_local, self.critic_target, TAU)\n",
    "        self.soft_update(self.actor_local, self.actor_target, TAU)                     \n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "            local_model: PyTorch model (weights will be copied from)\n",
    "            target_model: PyTorch model (weights will be copied to)\n",
    "            tau (float): interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)\n",
    "            \n",
    "    def hard_update(self,local_model,target_model):\n",
    "        \n",
    "        for target_param,local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(local_param.data)\n",
    "\n",
    "class OUNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size,seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
    "        x = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): maximum size of buffer\n",
    "            batch_size (int): size of each training batch\n",
    "        \"\"\"\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "snake1 = Agent(289,4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_decay_schedule = [epochs//3,epochs//5,epochs//20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNAKE 1 DIED!\n",
      "\n",
      "\n",
      "*******************\n",
      "**** GAME OVER ****\n",
      "******************* \n",
      "\n",
      "episode: 9999 max reward: 25\n",
      "{-50: 9490, -25: 482, 0: 26, 25: 1}\n"
     ]
    }
   ],
   "source": [
    "timesteps = 400\n",
    "epochs = 10000\n",
    "max_reward = 0\n",
    "rewards = {}\n",
    "action_frequency = {}\n",
    "action_frequency[0] = 0\n",
    "action_frequency[1] = 0\n",
    "action_frequency[2] = 0\n",
    "action_frequency[3] = 0\n",
    "game_images = {}\n",
    "epsilon = 1.\n",
    "final_epsilon = 0.01\n",
    "\n",
    "# epsilon_decay = 0.999\n",
    "for i in range(epochs):\n",
    "    \n",
    "    if i%10==0:\n",
    "        epsilon = max(final_epsilon, epsilon-0.001)\n",
    "    iterations = 0\n",
    "    env.reset(1)\n",
    "    images = []\n",
    "    while True:\n",
    "        a = tuple([snake1.act(env.grid,epsilon)])\n",
    "        action_frequency[a[0]] += 1\n",
    "        state,reward,done,_ = env.step(a)\n",
    "        iterations+=1\n",
    "        images.append(env.grid)\n",
    "#         if iterations%10==0:\n",
    "#             print(\"epoch:\",i)\n",
    "#             print(\"timestep:\",iterations)\n",
    "#             plt.imshow(state)\n",
    "#             plt.show()\n",
    "\n",
    "        if done or iterations>timesteps:\n",
    "            if reward[0]>max_reward:\n",
    "                max_reward = reward[0]\n",
    "                \n",
    "            print(\"episode:\",i,\"max reward:\",max_reward)\n",
    "            print(rewards)\n",
    "            if reward[0] not in rewards.keys():\n",
    "                rewards[reward[0]] = 1\n",
    "                game_images[reward[0]] = images\n",
    "            else:\n",
    "                rewards[reward[0]] += 1\n",
    "                game_images[reward[0]] = images\n",
    "            \n",
    "            clear_output(wait=True)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-38, -46, -44, -48, 76, -40, -47, -20, -26, -41, -49, 59, -19, -27, -24, -36, -31, 66, 64, -42, -37, -35, -17, 53, 70, 56, 67, -45, -34, -32, -43, 80, 257, -39, 58, -25, 165, 177, -30, 60, 157, 68, 57, 61, 69, -28, -18, 55, -23, 159, 106, 78, -33, 65, 82, -22, 62, 89, 63, 52, 75, 183, 54, 188, 163, 79, -29, 87, -13, 162, 72, 71, 84, 77, -21, 74]\n"
     ]
    }
   ],
   "source": [
    "print(list(rewards.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 19327, 1: 20022, 2: 19135, 3: 77802}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "* It is possible that after some time, due to the lack of difference between the values of the apple and the snake element, that the snake thinks that the closest fruit is in its body itself. A possible solution to that could be that we make the snake element and fruit element values to be drastically different. To avoid this confusion. (Done)\n",
    "\n",
    "* The reward structure can also be changed. Right now we start with 1 reward, don't penalise for dying and don't reward for staying alive, plus give reward of 1 for finding the fruit. The reward structure can be changed to heavily penalising death, giving more rewards for eating an apple, and also incentivise staying alive. \n",
    "\n",
    "* Plus we started with only MLP, which in my personal opinion is not scalable, as the size of the grid increases, it will be harder for the agent to process the complete space, and hence it is important for it to be able to derive features from the picture, and that can be done through a convnet. (Done)\n",
    "\n",
    "* Was making a mistake with the output of the critic network. The output should be the reward associated with the action taken by the agent on that state, rather than the max as is being done right now. (Done)\n",
    "\n",
    "* Was making a mistake with the sampling of the action from the probabilities as well. Needs to be corrected (Done)\n",
    "\n",
    "* Other things that can be done is of trying other algorithms, and also tuning the hyper parameters.\n",
    "\n",
    "* Also the complete pipeline needs to be reviewed for every logic and accordingly refactored if required.\n",
    "\n",
    "* Batch norm and dropout can also be tried with the use of Convnet.\n",
    "\n",
    "* Clear output is the best thing for inline jupyternotebook visualisation.\n",
    "\n",
    "* Visualisations for the inner layers of the convnet can be done.\n",
    "\n",
    "* Inner layers of the networks can be frozen for different environment size training.\n",
    "\n",
    "* Weight matrix initialisation might be an issue. Started with Xavier, will switch to Kaiming Initialization, which is a better initialisation algorithm for unsymmetric activation functions (Done)\n",
    "\n",
    "* If we incentivize being alive very well, then is it possible that the snake learns to just move in a loop, not caring about other rewards\n",
    "\n",
    "* Another way of passing the state information to the snake, is to specify the locations of all other elements in the environment. This method will have its own set of complexities that would have to be taken care of separately.\n",
    "\n",
    "* Simplifying the network to reduce effects of vanishing gradients, and regularization (Dropout, batchnorm, and extended connections might help here)\n",
    "\n",
    "* Although DDPG is capable of providing excellent results, it has its drawbacks. Like many RL algorithms training DDPG can be unstable and heavily reliant on finding the correct hyper parameters for the current task (OpenAI Spinning Up, 2018). This is caused by the algorithm continuously over estimating the Q values of the critic (value) network. These estimation errors build up over time and can lead to the agent falling into a local optima or experience catastrophic forgetting. TD3 addresses this issue by focusing on reducing the overestimation bias seen in previous algorithms. Hence TD3 also needs to be coded out and tried. This is done with the addition of 3 key features:\n",
    "    Using a pair of critic networks (The twin part of the title)\n",
    "    Delayed updates of the actor (The delayed part)\n",
    "    Action noise regularisation (This part didn’t make it to the title :/ )\n",
    "    \n",
    "* Checkpointing will also have to be done for the network, incase a minima is reached, and then later the network can be loaded and then be used to replicate performance.\n",
    "\n",
    "* Study between negative and positive feedback can also be done.\n",
    "\n",
    "* Kaiming initialisation is giving better performance as compared to other initialisations. Need to run seperate tests for these as well.\n",
    "\n",
    "* The final layer activation for the critic should be linear, since reward structure was changed to include negative rewards as well. elu is also an option here.\n",
    "\n",
    "* A proper schedule for the epsilon decay can be developed for a more apt exploration-expoilation use-case\n",
    "\n",
    "* Learning how to tune hyperparameters might be important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current working directory is /Users/championballer/Documents/Code/mgym/examples\n",
      "Successfully created the directory ./Output/output9 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "print (\"The current working directory is %s\" % path)\n",
    "\n",
    "path = \"./Output/output\"+str(output)\n",
    "\n",
    "try:\n",
    "    os.mkdir(path)\n",
    "except OSError:\n",
    "    print (\"Creation of the directory %s failed\" % path)\n",
    "else:\n",
    "    print (\"Successfully created the directory %s \" % path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open(\"./Output/output\"+str(output)+\"/README.md\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#f.write(\"17*17,100000,70,0.00001 eps decay, Reward structure modified\")\n",
    "\n",
    "for i in range(len(game_images[25])):\n",
    "\n",
    "    img = game_images[25][i]\n",
    "    plt.imshow(img,cmap='inferno')\n",
    "    #plt.savefig(\"./Output/output\"+str(output)+\"/plot\"+str(i)+\".png\")\n",
    "    plt.show()\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "#output+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0877,  0.0418,  0.0182, -0.0191], dtype=torch.float64,\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "action = Agent.forward(state)\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(4, p=[0.3, 0.2, 0.3, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
